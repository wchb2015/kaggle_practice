{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Algorithms - Classification Example\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "For this example, we are trying to predict if it will rain tomorrow based on weather data from Australia. This could be something that would be useful for a weather station or a website to project. \n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/weatherAUS.csv')\n",
    "df = df.iloc[:5000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 23)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
      "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
      "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
      "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
      "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
      "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
      "\n",
      "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
      "0           W           44.0          W  ...        71.0         22.0   \n",
      "1         WNW           44.0        NNW  ...        44.0         25.0   \n",
      "2         WSW           46.0          W  ...        38.0         30.0   \n",
      "3          NE           24.0         SE  ...        45.0         16.0   \n",
      "4           W           41.0        ENE  ...        82.0         33.0   \n",
      "\n",
      "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
      "0       1007.7       1007.1       8.0       NaN     16.9     21.8         No   \n",
      "1       1010.6       1007.8       NaN       NaN     17.2     24.3         No   \n",
      "2       1007.6       1008.7       NaN       2.0     21.0     23.2         No   \n",
      "3       1017.6       1012.8       NaN       NaN     18.1     26.5         No   \n",
      "4       1010.8       1006.0       7.0       8.0     17.8     29.7         No   \n",
      "\n",
      "   RainTomorrow  \n",
      "0            No  \n",
      "1            No  \n",
      "2            No  \n",
      "3            No  \n",
      "4            No  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 23 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Date           5000 non-null   object \n",
      " 1   Location       5000 non-null   object \n",
      " 2   MinTemp        4975 non-null   float64\n",
      " 3   MaxTemp        4981 non-null   float64\n",
      " 4   Rainfall       4917 non-null   float64\n",
      " 5   Evaporation    0 non-null      float64\n",
      " 6   Sunshine       0 non-null      float64\n",
      " 7   WindGustDir    4925 non-null   object \n",
      " 8   WindGustSpeed  4925 non-null   float64\n",
      " 9   WindDir9am     4153 non-null   object \n",
      " 10  WindDir3pm     4900 non-null   object \n",
      " 11  WindSpeed9am   4962 non-null   float64\n",
      " 12  WindSpeed3pm   4959 non-null   float64\n",
      " 13  Humidity9am    4970 non-null   float64\n",
      " 14  Humidity3pm    4966 non-null   float64\n",
      " 15  Pressure9am    4977 non-null   float64\n",
      " 16  Pressure3pm    4966 non-null   float64\n",
      " 17  Cloud9am       1289 non-null   float64\n",
      " 18  Cloud3pm       1427 non-null   float64\n",
      " 19  Temp9am        4970 non-null   float64\n",
      " 20  Temp3pm        4966 non-null   float64\n",
      " 21  RainToday      4917 non-null   object \n",
      " 22  RainTomorrow   4917 non-null   object \n",
      "dtypes: float64(16), object(7)\n",
      "memory usage: 898.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           MinTemp      MaxTemp     Rainfall  Evaporation  Sunshine  \\\n",
      "count  4975.000000  4981.000000  4917.000000          0.0       0.0   \n",
      "mean     10.017286    23.069183     1.999268          NaN       NaN   \n",
      "std       5.947947     7.120703     6.526700          NaN       NaN   \n",
      "min      -3.000000     6.800000     0.000000          NaN       NaN   \n",
      "25%       5.300000    17.400000     0.000000          NaN       NaN   \n",
      "50%       9.900000    22.400000     0.000000          NaN       NaN   \n",
      "75%      14.900000    28.300000     0.400000          NaN       NaN   \n",
      "max      28.300000    45.800000   116.000000          NaN       NaN   \n",
      "\n",
      "       WindGustSpeed  WindSpeed9am  WindSpeed3pm  Humidity9am  Humidity3pm  \\\n",
      "count    4925.000000   4962.000000   4959.000000  4970.000000  4966.000000   \n",
      "mean       33.352690      8.220073     14.428715    75.055332    49.024366   \n",
      "std        13.257268      6.770040      7.478199    17.173014    19.350449   \n",
      "min         7.000000      0.000000      0.000000    18.000000     7.000000   \n",
      "25%        24.000000      4.000000      9.000000    63.000000    35.000000   \n",
      "50%        31.000000      7.000000     13.000000    76.000000    48.000000   \n",
      "75%        41.000000     11.000000     19.000000    89.000000    61.000000   \n",
      "max       107.000000     48.000000     56.000000   100.000000   100.000000   \n",
      "\n",
      "       Pressure9am  Pressure3pm     Cloud9am     Cloud3pm      Temp9am  \\\n",
      "count  4977.000000  4966.000000  1289.000000  1427.000000  4970.000000   \n",
      "mean   1018.447840  1015.760753     6.391001     5.406447    15.098249   \n",
      "std       7.182578     7.029994     2.387684     2.832176     5.906911   \n",
      "min     989.800000   982.900000     0.000000     1.000000     0.300000   \n",
      "25%    1013.600000  1010.900000     5.000000     2.000000    10.400000   \n",
      "50%    1018.500000  1015.700000     8.000000     7.000000    15.200000   \n",
      "75%    1023.300000  1020.500000     8.000000     8.000000    19.600000   \n",
      "max    1039.900000  1036.800000     8.000000     8.000000    34.500000   \n",
      "\n",
      "           Temp3pm  \n",
      "count  4966.000000  \n",
      "mean     21.712767  \n",
      "std       6.920835  \n",
      "min       6.400000  \n",
      "25%      16.200000  \n",
      "50%      21.000000  \n",
      "75%      26.700000  \n",
      "max      43.700000  \n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics for numerical columns\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                0\n",
      "Location            0\n",
      "MinTemp            25\n",
      "MaxTemp            19\n",
      "Rainfall           83\n",
      "Evaporation      5000\n",
      "Sunshine         5000\n",
      "WindGustDir        75\n",
      "WindGustSpeed      75\n",
      "WindDir9am        847\n",
      "WindDir3pm        100\n",
      "WindSpeed9am       38\n",
      "WindSpeed3pm       41\n",
      "Humidity9am        30\n",
      "Humidity3pm        34\n",
      "Pressure9am        23\n",
      "Pressure3pm        34\n",
      "Cloud9am         3711\n",
      "Cloud3pm         3573\n",
      "Temp9am            30\n",
      "Temp3pm            34\n",
      "RainToday          83\n",
      "RainTomorrow       83\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create months column\n",
    "# figure out location data \n",
    "# what to do with imputed data \n",
    "# scale data \n",
    "#remove outliers \n",
    "\n",
    "## Naive Bayes \n",
    "    # Outlier removal \n",
    "    # Encoding \n",
    "## SVM & Logistic regression Logistic Regression\n",
    "    # Remove Outliers \n",
    "    # Remove missing values \n",
    "    # Scaling \n",
    "    # dummy variables \n",
    "## Trees (Decision, RF, XGBoost, GB Tree)\n",
    "## KNN\n",
    "    # Feature scaling\n",
    "    # Imputation \n",
    "    # dummy variables \n",
    "## ANN \n",
    "    # scaling \n",
    "    # get dummies "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning \n",
    "1) Drop Null Values in y variable\n",
    "2) Create a category for Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning \n",
    "df = df.dropna(subset=['RainTomorrow'])\n",
    "df['month'] = df.Date.apply(lambda x: pd.to_datetime(x).month).astype('category')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model baseline\n",
    "\n",
    "The dataset is slightly imbalanced. We see that if we project it not to rain every time, our model will be right around 78% of the time. We want our model to perform at least to perform better than this 78% threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "RainTomorrow",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "d83b29ab-cb69-4fee-bb9f-ab18dd3ff101",
       "rows": [
        [
         "No",
         "0.797437461866992"
        ],
        [
         "Yes",
         "0.20256253813300792"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "RainTomorrow\n",
       "No     0.797437\n",
       "Yes    0.202563\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['RainTomorrow'].value_counts()/ df.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Data Preprocessing\n",
    "1) Remove outliers\n",
    "2) Create train test split\n",
    "3) Create columns for continuous and categorical varaiables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from scipy import stats\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import scipy \n",
    "#adjust for X & Y \n",
    "\n",
    "def z_score_removal(X,y, columns, z_score):\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "    col_df = df[columns]    \n",
    "    z_scores = scipy.stats.zscore(col_df).abs()\n",
    "    outliers = (z_scores.max(axis=1) > z_score)\n",
    "    df_out = df[~outliers]\n",
    "    X_cleaned = df_out[X.columns]\n",
    "    y_cleaned = df_out.drop(X.columns, axis =1)\n",
    "    return X_cleaned, y_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split \n",
    "from sklearn.model_selection import train_test_split    \n",
    "X = df.drop(['RainTomorrow', 'Date'], axis=1)\n",
    "y = df.loc[:,'RainTomorrow'].map({'Yes': 1, 'No': 0}).astype('category')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify continuous and categorical columns\n",
    "continuous_columns = [col for col in X_train.columns if X_train[col].dtype == 'float64' or X_train[col].dtype == 'int64']\n",
    "categorical_columns = [col for col in X_train.columns if X_train[col].dtype == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am',\n",
      "       'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am',\n",
      "       'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm',\n",
      "       'Location_Albury', 'Location_BadgerysCreek', 'WindGustDir_E',\n",
      "       'WindGustDir_ENE', 'WindGustDir_ESE', 'WindGustDir_N', 'WindGustDir_NE',\n",
      "       'WindGustDir_NNE', 'WindGustDir_NNW', 'WindGustDir_NW', 'WindGustDir_S',\n",
      "       'WindGustDir_SE', 'WindGustDir_SSE', 'WindGustDir_SSW',\n",
      "       'WindGustDir_SW', 'WindGustDir_W', 'WindGustDir_WNW', 'WindGustDir_WSW',\n",
      "       'WindGustDir_nan', 'WindDir9am_E', 'WindDir9am_ENE', 'WindDir9am_ESE',\n",
      "       'WindDir9am_N', 'WindDir9am_NE', 'WindDir9am_NNE', 'WindDir9am_NNW',\n",
      "       'WindDir9am_NW', 'WindDir9am_S', 'WindDir9am_SE', 'WindDir9am_SSE',\n",
      "       'WindDir9am_SSW', 'WindDir9am_SW', 'WindDir9am_W', 'WindDir9am_WNW',\n",
      "       'WindDir9am_WSW', 'WindDir9am_nan', 'WindDir3pm_E', 'WindDir3pm_ENE',\n",
      "       'WindDir3pm_ESE', 'WindDir3pm_N', 'WindDir3pm_NE', 'WindDir3pm_NNE',\n",
      "       'WindDir3pm_NNW', 'WindDir3pm_NW', 'WindDir3pm_S', 'WindDir3pm_SE',\n",
      "       'WindDir3pm_SSE', 'WindDir3pm_SSW', 'WindDir3pm_SW', 'WindDir3pm_W',\n",
      "       'WindDir3pm_WNW', 'WindDir3pm_WSW', 'WindDir3pm_nan', 'RainToday_No',\n",
      "       'RainToday_Yes', 'RainToday_nan', 'month'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Evaporation' 'Sunshine']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# pipeline for Naive Bayes - We need to impute continuous columns and encode categorical variables.\n",
    "nb_preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "    ]), continuous_columns),\n",
    "    ('cat', Pipeline([\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ]), categorical_columns)\n",
    "],remainder = 'passthrough', verbose_feature_names_out= False).set_output(transform='pandas')\n",
    "\n",
    "data_transformed = nb_preprocessor.fit_transform(X_train)\n",
    "\n",
    "\n",
    "new_continuous_columns = [col for col in continuous_columns if col in data_transformed.columns]\n",
    "\n",
    "# Fit and transform the data\n",
    "print(data_transformed.columns)\n",
    "nb_X_train, nb_y_train = z_score_removal(data_transformed, y_train, new_continuous_columns, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Naive Bayes\n",
    "\n",
    "Naive Bayes is a family of simple probabilistic classifiers based on applying Bayes' theorem with the \"naive\" assumption of conditional independence between every pair of features given the class. These classifiers are particularly useful for text classification and other high-dimensional problems. There are different Naive Bayes classifiers available in scikit-learn, such as GaussianNB, MultinomialNB, and BernoulliNB.\n",
    "\n",
    "Relevant Parameters:\n",
    "\n",
    "### GaussianNB\n",
    "- **var_smoothing**: Portion of the largest variance of all features that is added to variances for calculation stability. It's used to smooth the likelihood estimates and avoid zero probabilities, which can lead to better generalization performance.\n",
    "\n",
    "### MultinomialNB\n",
    "- **alpha**: Additive (Laplace/Lidstone) smoothing parameter. It's used to control the trade-off between fitting the data and smoothing the probabilities, which helps prevent overfitting.\n",
    "- **fit_prior**: Whether to learn class prior probabilities or not. If false, a uniform prior will be used. Learning the prior can help improve the classification performance in cases where the class distribution is imbalanced.\n",
    "\n",
    "### BernoulliNB\n",
    "- **alpha**: Additive (Laplace/Lidstone) smoothing parameter. It's used to control the trade-off between fitting the data and smoothing the probabilities, which helps prevent overfitting.\n",
    "- **binarize**: Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors.\n",
    "- **fit_prior**: Whether to learn class prior probabilities or not. If false, a uniform prior will be used. Learning the prior can help improve the classification performance in cases where the class distribution is imbalanced.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76157205 0.76048951 0.75874126]\n",
      "Test score (accuracy): 0.7762889600932129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Code \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "nb_model = GaussianNB()\n",
    "nb_scores = cross_val_score(nb_model, nb_X_train, nb_y_train, cv=3)\n",
    "print(nb_scores)\n",
    "\n",
    "# No hyperparameters to tune for GaussianNB\n",
    "nb_model.fit(nb_X_train, nb_y_train)\n",
    "test_score_nb = nb_model.score(nb_X_train, nb_y_train)\n",
    "\n",
    "print(f\"Test score (accuracy): {test_score_nb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Evaporation' 'Sunshine']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# pipeline for SVM & Logistic regression classifiers \n",
    "lr_preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]), continuous_columns),\n",
    "    ('cat', Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ]), categorical_columns)\n",
    "],remainder = 'passthrough', verbose_feature_names_out= False).set_output(transform='pandas')\n",
    "\n",
    "lr_transformed = lr_preprocessor.fit_transform(X_train)\n",
    "\n",
    "new_continuous_columns = [col for col in continuous_columns if col in data_transformed.columns]\n",
    "\n",
    "\n",
    "lr_X_train, lr_y_train = z_score_removal(lr_transformed, y_train, new_continuous_columns, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Logistic Regression\n",
    "\n",
    "Logistic Regression is a linear model for classification that uses the logistic function to model the probability of a binary outcome. It's a simple yet powerful technique for solving binary and multi-class classification problems. In scikit-learn, the `LogisticRegression` class provides an implementation of logistic regression.\n",
    "\n",
    "Relevant Parameters:\n",
    "- **penalty**: The type of regularization applied to the model. Options include 'l1', 'l2', 'elasticnet', and 'none'. Regularization is used to control the trade-off between fitting the data and keeping the weights small, which helps prevent overfitting.\n",
    "- **C**: Inverse of regularization strength (i.e., 1/lambda). Smaller values specify stronger regularization. It's used to control the amount of regularization applied to the model, which can impact the model's ability to generalize to unseen data.\n",
    "- **fit_intercept**: Whether to include an intercept term in the model. If false, the data is assumed to be already centered. Including an intercept can improve the fit of the model, especially if the data is not centered.\n",
    "- **solver**: The algorithm used for optimization. Choices are 'newton-cg', 'lbfgs', 'liblinear', 'sag', and 'saga'. Each solver has its own benefits and drawbacks, so it's essential to choose the one that best suits your problem and dataset.\n",
    "- **max_iter**: Maximum number of iterations for the solver to converge. Increasing this value allows the model more time to converge but may increase the computation time.\n",
    "- **multi_class**: Strategy for multi-class problems. Options are 'auto', 'ovr' (one-vs-rest), and 'multinomial'. 'auto' will choose the best strategy based on the data and solver. For multi-class problems, the choice of strategy can impact the classification performance.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88296943 0.87674825 0.88723776]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Parameter grid for GridSearchCV\\nparam_grid = {\\n    \\'penalty\\': [\\'l1\\', \\'l2\\', \\'elasticnet\\', \\'none\\'],\\n    \\'C\\': [0.1, 1, 10],\\n    \\'solver\\': [\\'newton-cg\\', \\'lbfgs\\', \\'liblinear\\', \\'sag\\', \\'saga\\'],\\n    \\'max_iter\\': [100, 500, 1000]\\n}\\n\\ngrid_search_lr = GridSearchCV(lr_model, param_grid, scoring=\\'accuracy\\', cv=3, n_jobs=-1, verbose=1)\\ngrid_search_lr.fit(lr_X_train, lr_y_train)\\ntest_score_lr = grid_search_lr.best_estimator_.score(lr_X_train, lr_y_train)\\n\\nprint(f\"Best penalty value: {grid_search_lr.best_params_[\\'penalty\\']}\")\\nprint(f\"Best C value: {grid_search_lr.best_params_[\\'C\\']}\")\\nprint(f\"Best solver value: {grid_search_lr.best_params_[\\'solver\\']}\")\\nprint(f\"Best max_iter value: {grid_search_lr.best_params_[\\'max_iter\\']}\")\\nprint(f\"Best accuracy: {grid_search_lr.best_score_}\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression & SVM Code\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# Logistic Regression with default parameters\n",
    "lr_model = LogisticRegression()\n",
    "lr_scores = cross_val_score(lr_model, lr_X_train, lr_y_train, cv=3, scoring='accuracy')\n",
    "print(lr_scores)\n",
    "\"\"\"\n",
    "# Parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "grid_search_lr = GridSearchCV(lr_model, param_grid, scoring='accuracy', cv=3, n_jobs=-1, verbose=1)\n",
    "grid_search_lr.fit(lr_X_train, lr_y_train)\n",
    "test_score_lr = grid_search_lr.best_estimator_.score(lr_X_train, lr_y_train)\n",
    "\n",
    "print(f\"Best penalty value: {grid_search_lr.best_params_['penalty']}\")\n",
    "print(f\"Best C value: {grid_search_lr.best_params_['C']}\")\n",
    "print(f\"Best solver value: {grid_search_lr.best_params_['solver']}\")\n",
    "print(f\"Best max_iter value: {grid_search_lr.best_params_['max_iter']}\")\n",
    "print(f\"Best accuracy: {grid_search_lr.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - SVM Classification\n",
    "\n",
    "Support Vector Machine (SVM) Classification is a versatile machine learning algorithm that can be used for both linear and non-linear classification tasks. It aims to find the best-fitting hyperplane that has the largest distance (margin) between the support vectors and the hyperplane.\n",
    "\n",
    "Relevant Parameters:\n",
    "- **kernel**: Specifies the kernel function to be used in the algorithm. Possible options are 'linear', 'poly', 'rbf', 'sigmoid', and 'precomputed'. The choice of the kernel function depends on the nature of the data and the problem to be solved.\n",
    "- **C**: Regularization parameter (also called the cost parameter); must be a positive float. It determines the trade-off between achieving a low training error and a low testing error. In other words, it controls the balance between overfitting and underfitting. A smaller value of C creates a wider margin, which may result in more training errors but better generalization to the test data. A larger value of C creates a narrower margin, which may result in fewer training errors but poorer generalization to the test data.\n",
    "- **degree**: The degree of the polynomial kernel function ('poly'). Ignored by all other kernels. It is the degree of the polynomial used for the 'poly' kernel and determines the flexibility of the model.\n",
    "- **gamma**: Kernel coefficient for 'rbf', 'poly', and 'sigmoid'. If gamma is 'scale' (default), then it is calculated as 1 / (n_features * X.var()) for the input data X. If gamma is 'auto', then it is calculated as 1/n_features. A smaller gamma value will produce a more flexible model, while a larger gamma value will produce a more rigid model.\n",
    "- **coef0**: Independent term in the kernel function. It is only significant in 'poly' and 'sigmoid'. It controls the influence of higher degree terms in the polynomial and sigmoid kernels.\n",
    "- **shrinking**: Whether to use the shrinking heuristic. The shrinking heuristic is a technique used to speed up training by removing some of the support vectors that are not necessary for the final solution. True by default.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific classification problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8768559  0.88199301 0.88199301]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Parameter grid for GridSearchCV\\nparam_grid = {\\n    \\'kernel\\': [\\'linear\\', \\'rbf\\'],\\n    \\'C\\': [0.1, 1, 10],\\n    \\'gamma\\': [\\'scale\\', \\'auto\\', 0.1, 1, 10]\\n}\\n\\ngrid_search_svm = GridSearchCV(svm_model, param_grid, scoring=\\'accuracy\\', cv=5, n_jobs=-1, verbose=1)\\ngrid_search_svm.fit(lr_X_train, lr_y_train)\\ntest_score_svm = grid_search_svm.best_estimator_.score(lr_X_train, lr_y_train)\\n\\nprint(f\"Best kernel value: {grid_search_svm.best_params_[\\'kernel\\']}\")\\nprint(f\"Best C value: {grid_search_svm.best_params_[\\'C\\']}\")\\nprint(f\"Best gamma value: {grid_search_svm.best_params_[\\'gamma\\']}\")\\nprint(f\"Best accuracy: {grid_search_svm.best_score_}\")\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# SVM with default parameters\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_scores = cross_val_score(svm_model, lr_X_train, lr_y_train, cv=3, scoring='accuracy')\n",
    "print(svm_scores)\n",
    "\n",
    "\"\"\"\n",
    "# Parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search_svm = GridSearchCV(svm_model, param_grid, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_svm.fit(lr_X_train, lr_y_train)\n",
    "test_score_svm = grid_search_svm.best_estimator_.score(lr_X_train, lr_y_train)\n",
    "\n",
    "print(f\"Best kernel value: {grid_search_svm.best_params_['kernel']}\")\n",
    "print(f\"Best C value: {grid_search_svm.best_params_['C']}\")\n",
    "print(f\"Best gamma value: {grid_search_svm.best_params_['gamma']}\")\n",
    "print(f\"Best accuracy: {grid_search_svm.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Evaporation' 'Sunshine']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# pipeline for Trees (Decision, RF, XGBoost, GB Tree) - Reuse nb without outlier removval\n",
    "tree_X_train = nb_preprocessor.fit_transform(X_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Decision Tree\n",
    "\n",
    "Decision Trees are a popular machine learning algorithm used for both regression and classification tasks. They are easy to interpret and can naturally handle a mixture of continuous and categorical variables.\n",
    "\n",
    "Relevant Parameters:\n",
    "- **criterion**: The function to measure the quality of a split. Supported criteria for regression are 'mse' (mean squared error) and 'friedman_mse' (improvement in mean squared error). For classification, supported criteria are 'gini' and 'entropy'.\n",
    "- **splitter**: The strategy used to choose the split at each node. Supported strategies are 'best' to choose the best split and 'random' to choose the best random split.\n",
    "- **max_depth**: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Controlling the depth can help prevent overfitting.\n",
    "- **min_samples_split**: The minimum number of samples required to split an internal node. A larger value prevents the tree from growing too deep, thus preventing overfitting.\n",
    "- **min_samples_leaf**: The minimum number of samples required to be at a leaf node. A larger value prevents the tree from growing too deep, thus preventing overfitting.\n",
    "- **min_weight_fraction_leaf**: The minimum weighted fraction of the sum total of weights required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "- **max_features**: The number of features to consider when looking for the best split. If None, then max_features=n_features.\n",
    "- **max_leaf_nodes**: Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None, then unlimited number of leaf nodes.\n",
    "- **min_impurity_decrease**: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "- **min_impurity_split**: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82074752 0.82608696 0.82303585]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Expanded parameter grid for GridSearchCV\\nparam_grid = {\\n    \\'criterion\\': [\\'gini\\', \\'entropy\\'],\\n    \\'splitter\\': [\\'best\\', \\'random\\'],\\n    \\'max_depth\\': [None, 5, 10, 15, 20, 25, 30, 35, 40],\\n    \\'min_samples_split\\': [2, 5, 10, 15, 20],\\n    \\'min_samples_leaf\\': [1, 2, 4, 6, 8, 10],\\n    \\'min_weight_fraction_leaf\\': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\\n    \\'max_features\\': [None, \\'sqrt\\', \\'log2\\'],\\n    \\'max_leaf_nodes\\': [None, 10, 20, 30, 40, 50],\\n    \\'min_impurity_decrease\\': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\\n    \\'class_weight\\': [None, \\'balanced\\']\\n}\\n\\ngrid_search_dt = GridSearchCV(dt_model, param_grid, scoring=\\'accuracy\\', cv=5, n_jobs=-1, verbose=1)\\ngrid_search_dt.fit(tree_X_train, y_train)\\ntest_score_dt = grid_search_dt.best_estimator_.score(tree_X_train, y_train)\\n\\nprint(\"Best hyperparameters found:\")\\nfor key, value in grid_search_dt.best_params_.items():\\n    print(f\"{key}: {value}\")\\n\\nprint(f\"Best accuracy: {grid_search_dt.best_score_}\")\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trees (Decision, RF, XGBoost, GB Tree) Code\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# Decision Tree with default parameters\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_scores = cross_val_score(dt_model, tree_X_train, y_train, cv=3, scoring='accuracy')\n",
    "print(dt_scores)\n",
    "\n",
    "\"\"\"\n",
    "# Expanded parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 5, 10, 15, 20, 25, 30, 35, 40],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8, 10],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30, 40, 50],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "grid_search_dt = GridSearchCV(dt_model, param_grid, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_dt.fit(tree_X_train, y_train)\n",
    "test_score_dt = grid_search_dt.best_estimator_.score(tree_X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "for key, value in grid_search_dt.best_params_.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"Best accuracy: {grid_search_dt.best_score_}\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Random Forest\n",
    "\n",
    "Random Forest is an ensemble learning method that constructs a multitude of decision trees at training time and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is highly flexible and can handle a wide variety of tasks.\n",
    "\n",
    "Relevant Parameters:\n",
    "- **n_estimators**: The number of trees in the forest. Increasing the number of trees can improve the model's performance, but may also increase the computation time.\n",
    "- **criterion**: The function to measure the quality of a split. Supported criteria for regression are 'mse' (mean squared error) and 'mae' (mean absolute error). For classification, supported criteria are 'gini' and 'entropy'.\n",
    "- **max_depth**: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Controlling the depth can help prevent overfitting.\n",
    "- **min_samples_split**: The minimum number of samples required to split an internal node. A larger value prevents the tree from growing too deep, thus preventing overfitting.\n",
    "- **min_samples_leaf**: The minimum number of samples required to be at a leaf node. A larger value prevents the tree from growing too deep, thus preventing overfitting.\n",
    "- **min_weight_fraction_leaf**: The minimum weighted fraction of the sum total of weights required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "- **max_features**: The number of features to consider when looking for the best split. If None, then max_features=n_features. It can also be a float, int, or string ('auto', 'sqrt', or 'log2').\n",
    "- **max_leaf_nodes**: Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None, then unlimited number of leaf nodes.\n",
    "- **min_impurity_decrease**: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "- **bootstrap**: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n",
    "- **oob_score**: Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86727689 0.85812357 0.8733791 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Expanded parameter grid for GridSearchCV\\nparam_grid = {\\n    \\'n_estimators\\': [10, 50, 100, 200, 300],\\n    \\'criterion\\': [\\'gini\\', \\'entropy\\'],\\n    \\'max_depth\\': [None, 5, 10, 15, 20, 25, 30, 35, 40],\\n    \\'min_samples_split\\': [2, 5, 10, 15, 20],\\n    \\'min_samples_leaf\\': [1, 2, 4, 6, 8, 10],\\n    \\'min_weight_fraction_leaf\\': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\\n    \\'max_features\\': [None, \\'sqrt\\', \\'log2\\'],\\n    \\'max_leaf_nodes\\': [None, 10, 20, 30, 40, 50],\\n    \\'min_impurity_decrease\\': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\\n    \\'bootstrap\\': [True, False],\\n    \\'class_weight\\': [None, \\'balanced\\'],\\n    \\'warm_start\\': [False, True],\\n    \\'oob_score\\': [False, True]\\n}\\n\\ngrid_search_rf = GridSearchCV(rf_model, param_grid, scoring=\\'accuracy\\', cv=5, n_jobs=-1, verbose=1)\\ngrid_search_rf.fit(tree_X_train, y_train)\\ntest_score_rf = grid_search_rf.best_estimator_.score(tree_X_train, y_train)\\n\\nprint(\"Best hyperparameters found:\")\\nfor key, value in grid_search_rf.best_params_.items():\\n    print(f\"{key}: {value}\")\\n\\nprint(f\"Best accuracy: {grid_search_rf.best_score_}\")\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# Random Forest with default parameters\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_scores = cross_val_score(rf_model, tree_X_train, y_train, cv=3, scoring='accuracy')\n",
    "print(rf_scores)\n",
    "\n",
    "\"\"\"\n",
    "# Expanded parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200, 300],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10, 15, 20, 25, 30, 35, 40],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8, 10],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30, 40, 50],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'warm_start': [False, True],\n",
    "    'oob_score': [False, True]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf_model, param_grid, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_rf.fit(tree_X_train, y_train)\n",
    "test_score_rf = grid_search_rf.best_estimator_.score(tree_X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "for key, value in grid_search_rf.best_params_.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"Best accuracy: {grid_search_rf.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Gradient Boosted Classifier\n",
    "\n",
    "Gradient Boosting is an ensemble learning method that builds an additive model in a forward stage-wise fashion. It allows for the optimization of arbitrary differentiable loss functions. In each stage, a regression tree is fit on the negative gradient of the given loss function.\n",
    "\n",
    "Relevant Parameters:\n",
    "- **loss**: The loss function to be optimized. For classification, supported options are 'deviance' (default) for the exponential loss and 'exponential' for AdaBoost-like exponential loss.\n",
    "- **learning_rate**: The learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.\n",
    "- **n_estimators**: The number of boosting stages to perform. Gradient boosting is fairly robust to overfitting, so a large number of estimators usually results in better performance.\n",
    "- **subsample**: The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0, this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.\n",
    "- **criterion**: The function to measure the quality of a split. Supported criteria are 'friedman_mse' (default) for the mean squared error with improvement score by Friedman, 'mse' for mean squared error, and 'mae' for the mean absolute error.\n",
    "- **min_samples_split**: The minimum number of samples required to split an internal node. A larger value prevents the tree from growing too deep, thus preventing overfitting.\n",
    "- **min_samples_leaf**: The minimum number of samples required to be at a leaf node. A larger value prevents the tree from growing too deep, thus preventing overfitting.\n",
    "- **min_weight_fraction_leaf**: The minimum weighted fraction of the sum total of weights required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "- **max_depth**: The maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.\n",
    "- **min_impurity_decrease**: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "- **max_features**: The number of features to consider when looking for the best split. If None, then max_features=n_features. It can also be a float, int, or string ('auto', 'sqrt', or 'log2').\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86651411 0.86346301 0.87643021]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Expanded parameter grid for GridSearchCV\\nparam_grid = {\\n    \\'loss\\': [\\'deviance\\', \\'exponential\\'],\\n    \\'learning_rate\\': [0.01, 0.1, 0.2, 0.3],\\n    \\'n_estimators\\': [10, 50, 100, 200, 300],\\n    \\'subsample\\': [0.5, 0.8, 1.0],\\n    \\'criterion\\': [\\'friedman_mse\\', \\'mse\\', \\'mae\\'],\\n    \\'min_samples_split\\': [2, 5, 10, 15, 20],\\n    \\'min_samples_leaf\\': [1, 2, 4, 6, 8, 10],\\n    \\'min_weight_fraction_leaf\\': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\\n    \\'max_depth\\': [None, 5, 10, 15, 20, 25, 30, 35, 40],\\n    \\'min_impurity_decrease\\': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\\n    \\'max_features\\': [None, \\'sqrt\\', \\'log2\\'],\\n    \\'max_leaf_nodes\\': [None, 10, 20, 30, 40, 50],\\n    \\'n_iter_no_change\\': [None, 5, 10, 15],\\n    \\'validation_fraction\\': [0.1, 0.2, 0.3, 0.4, 0.5],\\n    \\'tol\\': [1e-4, 1e-3, 1e-2, 1e-1]\\n}\\n\\ngrid_search_gb = GridSearchCV(gb_model, param_grid, scoring=\\'accuracy\\', cv=5, n_jobs=-1, verbose=1)\\ngrid_search_gb.fit(tree_X_train, y_train)\\ntest_score_gb = grid_search_gb.best_estimator_.score(tree_X_train, y_train)\\n\\nprint(\"Best hyperparameters found:\")\\nfor key, value in grid_search_gb.best_params_.items():\\n    print(f\"{key}: {value}\")\\n\\nprint(f\"Best accuracy: {grid_search_gb.best_score_}\")\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# Gradient Boosting with default parameters\n",
    "gb_model = GradientBoostingClassifier()\n",
    "gb_scores = cross_val_score(gb_model, tree_X_train, y_train, cv=3, scoring='accuracy')\n",
    "print(gb_scores)\n",
    "\n",
    "\"\"\"\n",
    "# Expanded parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'loss': ['deviance', 'exponential'],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [10, 50, 100, 200, 300],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8, 10],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'max_depth': [None, 5, 10, 15, 20, 25, 30, 35, 40],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30, 40, 50],\n",
    "    'n_iter_no_change': [None, 5, 10, 15],\n",
    "    'validation_fraction': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'tol': [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(gb_model, param_grid, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_gb.fit(tree_X_train, y_train)\n",
    "test_score_gb = grid_search_gb.best_estimator_.score(tree_X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "for key, value in grid_search_gb.best_params_.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"Best accuracy: {grid_search_gb.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - XGBoost\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework, offering several regularization techniques to prevent overfitting.\n",
    "\n",
    "Relevant Parameters:\n",
    "- **learning_rate**: Boosting learning rate. Controls the contribution of each tree in the ensemble. Lower learning rates lead to more robust models but require more trees (n_estimators).\n",
    "- **n_estimators**: Number of boosting rounds to be run. Larger values result in more complex models but can increase the risk of overfitting.\n",
    "- **max_depth**: Maximum tree depth for base learners. Controls the depth of each individual tree in the ensemble. Deeper trees can capture more complex patterns, but may also overfit the data.\n",
    "- **min_child_weight**: Minimum sum of instance weight (hessian) needed in a child. Defines the minimum number of instances required for a node to be split.\n",
    "- **gamma**: Minimum loss reduction required to make a further partition on a leaf node of the tree. Controls the complexity of the tree by reducing the number of splits made.\n",
    "- **subsample**: Subsample ratio of the training instances. Setting it to a value less than 1.0 can help prevent overfitting.\n",
    "- **colsample_bytree**: Subsample ratio of columns when constructing each tree. A smaller value can reduce overfitting and speed up the training process.\n",
    "- **colsample_bylevel**: Subsample ratio of columns for each level. Specifies the fraction of features to choose for each level in the tree building process.\n",
    "- **colsample_bynode**: Subsample ratio of columns for each split. Specifies the fraction of features to choose for each split in the tree building process.\n",
    "- **reg_alpha**: L1 regularization term on weights. Controls the sparsity of feature weights, effectively performing feature selection.\n",
    "- **reg_lambda**: L2 regularization term on weights. Smoothens the weights, preventing extreme values and reducing the risk of overfitting.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:18:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:18:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86575133 0.85583524 0.86575133]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:18:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Expanded parameter grid for GridSearchCV\\nparam_grid = {\\n    \\'max_depth\\': [3, 6, 9, 12],\\n    \\'learning_rate\\': [0.01, 0.1, 0.2, 0.3],\\n    \\'n_estimators\\': [10, 50, 100, 200, 300],\\n    \\'booster\\': [\\'gbtree\\', \\'gblinear\\', \\'dart\\'],\\n    \\'min_child_weight\\': [1, 5, 10],\\n    \\'gamma\\': [0, 0.1, 0.2, 0.3, 0.4],\\n    \\'subsample\\': [0.5, 0.8, 1.0],\\n    \\'colsample_bytree\\': [0.5, 0.8, 1.0],\\n    \\'colsample_bylevel\\': [0.5, 0.8, 1.0],\\n    \\'reg_alpha\\': [0, 0.1, 0.2, 0.3, 0.4],\\n    \\'reg_lambda\\': [1, 2, 3, 4],\\n    \\'scale_pos_weight\\': [1, 2, 3],\\n    \\'max_delta_step\\': [0, 1, 2, 3, 4],\\n    \\'base_score\\': [0.5, 0.6, 0.7, 0.8, 0.9],\\n    \\'random_state\\': [0, 1, 2, 3]\\n}\\n\\ngrid_search_xgb = GridSearchCV(xgb_model, param_grid, scoring=\\'accuracy\\', cv=5, n_jobs=-1, verbose=1)\\ngrid_search_xgb.fit(tree_X_train, y_train)\\ntest_score_xgb = grid_search_xgb.best_estimator_.score(tree_X_train, y_train)\\n\\nprint(\"Best hyperparameters found:\")\\nfor key, value in grid_search_xgb.best_params_.items():\\n    print(f\"{key}: {value}\")\\n\\nprint(f\"Best accuracy: {grid_search_xgb.best_score_}\")\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# XGBoost with default parameters\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False)\n",
    "xgb_scores = cross_val_score(xgb_model, tree_X_train.values, y_train.values, cv=3, scoring='accuracy')\n",
    "print(xgb_scores)\n",
    "\"\"\"\n",
    "# Expanded parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3, 6, 9, 12],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [10, 50, 100, 200, 300],\n",
    "    'booster': ['gbtree', 'gblinear', 'dart'],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'colsample_bylevel': [0.5, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'reg_lambda': [1, 2, 3, 4],\n",
    "    'scale_pos_weight': [1, 2, 3],\n",
    "    'max_delta_step': [0, 1, 2, 3, 4],\n",
    "    'base_score': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'random_state': [0, 1, 2, 3]\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(xgb_model, param_grid, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_xgb.fit(tree_X_train, y_train)\n",
    "test_score_xgb = grid_search_xgb.best_estimator_.score(tree_X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "for key, value in grid_search_xgb.best_params_.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"Best accuracy: {grid_search_xgb.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline for KNN & ANN - Same as lr but wihout outlier removal\n",
    "knn_X_train = lr_transformed.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Nearest Neighbors (KNN) is a simple, yet powerful, non-parametric supervised learning algorithm used for classification and regression. It assigns a new instance to the majority class or computes the mean (for regression tasks) of its k nearest neighbors in the feature space.\n",
    "\n",
    "Relevant Parameters:\n",
    "- **n_neighbors**: Number of neighbors to use for the query. This is the main hyperparameter controlling the complexity of the KNN model. Larger values of k lead to smoother decision boundaries, while smaller values can capture more complex patterns but may overfit the data.\n",
    "- **weights**: Weight function used in prediction. There are two options: 'uniform' (all points in each neighborhood are weighted equally) and 'distance' (assign weights proportional to the inverse of the distance from the query point). Using 'distance' can help reduce the impact of noise in the data.\n",
    "- **algorithm**: Algorithm used to compute the nearest neighbors. Options include 'auto', 'ball_tree', 'kd_tree', and 'brute'. 'auto' will attempt to decide the most appropriate algorithm based on the values passed to fit() method. Choose the algorithm that best suits your data and computational requirements.\n",
    "- **leaf_size**: Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n",
    "- **p**: Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and for p = 2, it's equivalent to using euclidean_distance (l2). A larger value of p can help capture the specific geometry of your feature space.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84820748 0.84973303 0.84973303]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Expanded parameter grid for GridSearchCV\\nparam_grid = {\\n    \\'n_neighbors\\': list(range(1, 31)),\\n    \\'weights\\': [\\'uniform\\', \\'distance\\'],\\n    \\'algorithm\\': [\\'auto\\', \\'ball_tree\\', \\'kd_tree\\', \\'brute\\'],\\n    \\'leaf_size\\': list(range(1, 50)),\\n    \\'p\\': [1, 2],\\n    \\'metric\\': [\\'euclidean\\', \\'manhattan\\', \\'chebyshev\\', \\'minkowski\\', \\'wminkowski\\', \\'seuclidean\\', \\'mahalanobis\\']\\n}\\n\\ngrid_search_knn = GridSearchCV(knn_model, param_grid, scoring=\\'accuracy\\', cv=5, n_jobs=-1, verbose=1)\\ngrid_search_knn.fit(knn_X_train, y_train)\\ntest_score_knn = grid_search_knn.best_estimator_.score(knn_X_train, y_train)\\n\\nprint(\"Best hyperparameters found:\")\\nfor key, value in grid_search_knn.best_params_.items():\\n    print(f\"{key}: {value}\")\\n\\nprint(f\"Best accuracy: {grid_search_knn.best_score_}\")\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN Code\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_scores = cross_val_score(knn_model, knn_X_train, y_train, cv=3, scoring='accuracy')\n",
    "print(knn_scores)\n",
    "\n",
    "\"\"\"\n",
    "# Expanded parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 31)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': list(range(1, 50)),\n",
    "    'p': [1, 2],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski', 'wminkowski', 'seuclidean', 'mahalanobis']\n",
    "}\n",
    "\n",
    "grid_search_knn = GridSearchCV(knn_model, param_grid, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_knn.fit(knn_X_train, y_train)\n",
    "test_score_knn = grid_search_knn.best_estimator_.score(knn_X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "for key, value in grid_search_knn.best_params_.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"Best accuracy: {grid_search_knn.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - MLPClassifier\n",
    "\n",
    "Multi-layer Perceptron (MLP) is a class of feedforward artificial neural network that can be used for classification and regression tasks. It consists of multiple layers of nodes, where each layer is fully connected to the next one. MLPClassifier is a popular implementation in scikit-learn for solving classification problems.\n",
    "\n",
    "Relevant Parameters:\n",
    "- **hidden_layer_sizes**: A tuple representing the number of neurons in each hidden layer. By adjusting this parameter, you can control the complexity of the model. Adding more hidden layers and neurons can increase the capacity of the model to learn complex patterns but may also lead to overfitting.\n",
    "- **activation**: Activation function for the hidden layers. Options include 'identity', 'logistic' (sigmoid), 'tanh', and 'relu'. Different activation functions can lead to different model behaviors and convergence properties.\n",
    "- **solver**: The solver for weight optimization. Choices are 'lbfgs', 'sgd', and 'adam'. Each solver has its own benefits and drawbacks, so it's essential to choose the one that best suits your problem and dataset.\n",
    "- **alpha**: L2 penalty (regularization term) parameter. It's used to control the trade-off between fitting the data and keeping the weights small, which helps prevent overfitting.\n",
    "- **batch_size**: The size of mini-batches for stochastic optimizers. If the solver is 'lbfgs', the classifier will not use mini-batch. For 'sgd' and 'adam', using smaller batch sizes can provide a regularizing effect but may increase the time required for convergence.\n",
    "- **learning_rate**: Learning rate schedule for weight updates. Options are 'constant', 'invscaling', and 'adaptive'. The learning rate determines how quickly the model adapts to the data, with larger values leading to faster convergence but potentially oscillating around the optimum.\n",
    "- **max_iter**: Maximum number of iterations. The solver iterates until convergence or this number of iterations is reached. Increasing this value allows the model more time to converge but may increase the computation time.\n",
    "\n",
    "By tuning these parameters, you can find the best combination for your specific problem and achieve a better balance between model complexity and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86193745 0.85507246 0.84515637]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Parameter grid for GridSearchCV\\nparam_grid = {\\n    \\'hidden_layer_sizes\\': [(10,), (20,), (50,), (10, 10), (20, 20), (50, 50)],\\n    \\'activation\\': [\\'identity\\', \\'logistic\\', \\'tanh\\', \\'relu\\'],\\n    \\'solver\\': [\\'lbfgs\\', \\'sgd\\', \\'adam\\'],\\n    \\'alpha\\': [0.0001, 0.001, 0.01, 0.1],\\n    \\'learning_rate\\': [\\'constant\\', \\'invscaling\\', \\'adaptive\\'],\\n    \\'max_iter\\': [200, 500, 1000],\\n}\\n\\ngrid_search_mlp = GridSearchCV(mlp_model, param_grid, scoring=\\'accuracy\\', cv=5, n_jobs=-1, verbose=1)\\ngrid_search_mlp.fit(knn_X_train, y_train)\\ntest_score_mlp = grid_search_mlp.best_estimator_.score(knn_X_train, y_train)\\n\\nprint(\"Best hyperparameters found:\")\\nfor key, value in grid_search_mlp.best_params_.items():\\n    print(f\"{key}: {value}\")\\n\\nprint(f\"Best accuracy: {grid_search_mlp.best_score_}\")\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANN Code\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# MLP with default parameters\n",
    "mlp_model = MLPClassifier(random_state=42)\n",
    "mlp_scores = cross_val_score(mlp_model, knn_X_train, y_train, cv=3, scoring='accuracy')\n",
    "print(mlp_scores)\n",
    "\n",
    "\"\"\"\n",
    "# Parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,), (20,), (50,), (10, 10), (20, 20), (50, 50)],\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'max_iter': [200, 500, 1000],\n",
    "}\n",
    "\n",
    "grid_search_mlp = GridSearchCV(mlp_model, param_grid, scoring='accuracy', cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search_mlp.fit(knn_X_train, y_train)\n",
    "test_score_mlp = grid_search_mlp.best_estimator_.score(knn_X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "for key, value in grid_search_mlp.best_params_.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"Best accuracy: {grid_search_mlp.best_score_}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Evaporation' 'Sunshine']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Evaporation' 'Sunshine']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_test_Nb = nb_preprocessor.transform(X_test).values\n",
    "X_test_LR = lr_preprocessor.transform(X_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:18:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(random_state=42)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model.fit(nb_X_train, nb_y_train)\n",
    "lr_model.fit(lr_X_train, lr_y_train)\n",
    "svm_model.fit(lr_X_train, lr_y_train)\n",
    "dt_model.fit(tree_X_train, y_train)\n",
    "rf_model.fit(tree_X_train, y_train)\n",
    "gb_model.fit(tree_X_train, y_train)\n",
    "xgb_model.fit(tree_X_train.values, y_train.values)\n",
    "knn_model.fit(knn_X_train, y_train)\n",
    "mlp_model.fit(knn_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GaussianNB was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.7266260162601627\n",
      "Logistic Regression Accuracy: 0.8861788617886179\n",
      "SVM Classification Accuracy: 0.875\n",
      "Decision Tree Accuracy: 0.8170731707317073\n",
      "Random Forest Accuracy: 0.8729674796747967\n",
      "Gradient Boosted Classifier Accuracy: 0.8821138211382114\n",
      "XGBoost Accuracy: 0.8678861788617886\n",
      "KNN Accuracy: 0.8597560975609756\n",
      "ANN Accuracy: 0.866869918699187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chongbei/Workspace/python/initial-bid/.venv_initial_bid/lib/python3.9/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Naive Bayes\n",
    "y_pred_nb = nb_model.predict(X_test_Nb)\n",
    "acc_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(f\"Naive Bayes Accuracy: {acc_nb}\")\n",
    "\n",
    "# Logistic Regression\n",
    "y_pred_lr = lr_model.predict(X_test_LR)\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Logistic Regression Accuracy: {acc_lr}\")\n",
    "\n",
    "# SVM Classification\n",
    "y_pred_svm = svm_model.predict(X_test_LR)\n",
    "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Classification Accuracy: {acc_svm}\")\n",
    "\n",
    "# Decision Tree\n",
    "y_pred_dt = dt_model.predict(X_test_Nb)\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f\"Decision Tree Accuracy: {acc_dt}\")\n",
    "\n",
    "# Random Forest\n",
    "y_pred_rf = rf_model.predict(X_test_Nb)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {acc_rf}\")\n",
    "\n",
    "# Gradient Boosted Classifier\n",
    "y_pred_gb = gb_model.predict(X_test_Nb)\n",
    "acc_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f\"Gradient Boosted Classifier Accuracy: {acc_gb}\")\n",
    "\n",
    "# XGBoost\n",
    "y_pred_xgb = xgb_model.predict(X_test_Nb)\n",
    "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {acc_xgb}\")\n",
    "\n",
    "# KNN\n",
    "y_pred_knn = knn_model.predict(X_test_LR)\n",
    "acc_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"KNN Accuracy: {acc_knn}\")\n",
    "\n",
    "# ANN\n",
    "y_pred_ann = mlp_model.predict(X_test_LR)\n",
    "acc_ann = accuracy_score(y_test, y_pred_ann)\n",
    "print(f\"ANN Accuracy: {acc_ann}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark(Initialbid)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "308e8e5829e870726ac9a9b610bce15efe1b382ea09a414e81be3a16d394eb2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
